{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Aggregate and save to parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1. Aggregate tenant counts in different countries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.sql.expressions.Window\n",
        "// paths\n",
        "val deltaTablesPath = \"abfss://xxx.dfs.core.windows.net/deltaTables\"\n",
        "val parquetsPath = \"abfss://xxx.dfs.core.windows.net/countryParquet\"\n",
        "\n",
        "val currentSnapshot = spark.read.format(\"delta\").load(deltaTablesPath)\n",
        "\n",
        "// step 01: deduplicate\n",
        "val w = Window.partitionBy(\"id\").orderBy(desc(\"createTime\"))\n",
        "val dudupDF = currentSnapshot.withColumn(\"rank\",dense_rank().over(w)).\n",
        "    where(col(\"rank\") === 1).drop(col(\"rank\"))\n",
        "\n",
        "// step 02: aggregation\n",
        "val aggregationResult = dudupDF.\n",
        "    where(col(\"licenseStatus\") === \"Active\").\n",
        "    groupBy(col(\"country\")).count()\n",
        "\n",
        "// step 03: save as parquet\n",
        "aggregationResult.coalesce(1).\n",
        "    write.mode(\"overwrite\").parquet(parquetsPath)\n",
        "\n",
        "val folderPath = \"abfss://xxx.dfs.core.windows.net/countryParquet/\"\n",
        "val files = mssparkutils.fs.ls(folderPath)\n",
        "\n",
        "val oldParquetName = folderPath + files.filter(_.name.endsWith(\"parquet\"))(0).name\n",
        "val newParquetName = folderPath + \"countryForBi.parquet\"\n",
        "\n",
        "mssparkutils.fs.cp(oldParquetName, newParquetName)\n",
        "mssparkutils.fs.rm(oldParquetName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2. Aggregate active/disabled tenant counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "// paths\n",
        "val deltaTablesPath = \"abfss://xxx.dfs.core.windows.net/deltaTables\"\n",
        "val parquetsPath = \"abfss://xxx.dfs.core.windows.net/statusParquet\"\n",
        "\n",
        "val currentSnapshot = spark.read.format(\"delta\").load(deltaTablesPath)\n",
        "\n",
        "// step 01: deduplicate\n",
        "val w = Window.partitionBy(\"id\").orderBy(desc(\"createTime\"))\n",
        "val dudupDF = currentSnapshot.withColumn(\"rank\",dense_rank().over(w)).\n",
        "    where(col(\"rank\") === 1).drop(col(\"rank\"))\n",
        "\n",
        "// step 02: aggregation\n",
        "val aggregationResult = dudupDF.groupBy(col(\"licenseStatus\")).count()\n",
        "\n",
        "// step 03: save as parquet\n",
        "aggregationResult.coalesce(1).\n",
        "    write.mode(\"overwrite\").parquet(parquetsPath)\n",
        "\n",
        "val folderPath = \"abfss://xxx.dfs.core.windows.net/statusParquet/\"\n",
        "val files = mssparkutils.fs.ls(folderPath)\n",
        "\n",
        "val oldParquetName = folderPath + files.filter(_.name.endsWith(\"parquet\"))(0).name\n",
        "val newParquetName = folderPath + \"statusForBi.parquet\"\n",
        "\n",
        "mssparkutils.fs.cp(oldParquetName, newParquetName)\n",
        "mssparkutils.fs.rm(oldParquetName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3. Aggregate onboard tenant counts every minute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "// paths\n",
        "val deltaTablesPath = \"abfss://xxx.dfs.core.windows.net/deltaTables\"\n",
        "val parquetsPath = \"abfss://xxx.dfs.core.windows.net/minuteCountParquet\"\n",
        "\n",
        "val currentSnapshot = spark.read.format(\"delta\").load(deltaTablesPath)\n",
        "\n",
        "// step 01: do aggregation w/o dedup\n",
        "val aggregationResult = currentSnapshot.where(col(\"licenseStatus\") === \"Active\").\n",
        "    groupBy(window(col(\"createTime\"), \"60 seconds\")).\n",
        "    agg(count(\"id\") as \"tenantOnboardCount\").\n",
        "    select(\"window.start\", \"window.end\", \"tenantOnboardCount\").\n",
        "    withColumn(\"intervalTime\",col(\"start\") + expr(\"INTERVAL 30 seconds\")).\n",
        "    drop(\"start\", \"end\")\n",
        "\n",
        "// step 02: save as parquet\n",
        "aggregationResult.coalesce(1).\n",
        "    write.mode(\"overwrite\").parquet(parquetsPath)\n",
        "\n",
        "val folderPath = \"abfss://xxx.dfs.core.windows.net/minuteCountParquet/\"\n",
        "val files = mssparkutils.fs.ls(folderPath)\n",
        "\n",
        "val oldParquetName = folderPath + files.filter(_.name.endsWith(\"parquet\"))(0).name\n",
        "val newParquetName = folderPath + \"minuteCountForBi.parquet\"\n",
        "\n",
        "mssparkutils.fs.cp(oldParquetName, newParquetName)\n",
        "mssparkutils.fs.rm(oldParquetName)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse Spark",
      "name": "synapse_spark"
    },
    "language_info": {
      "name": "scala"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
