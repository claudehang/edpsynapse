{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Streaming data from Cosmos DB to spark delta table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-12T06:37:41.4658161Z",
              "execution_start_time": "2021-10-12T02:57:57.1126058Z",
              "livy_statement_state": "cancelling",
              "queued_time": "2021-10-12T02:55:32.7359537Z",
              "session_id": 8,
              "session_start_time": "2021-10-12T02:55:32.7716907Z",
              "spark_pool": "smallpool",
              "state": "cancelled",
              "statement_id": 1
            },
            "text/plain": [
              "StatementMeta(smallpool, 8, 1, Cancelled, Cancelling)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "// paths\n",
        "val deltaCheckpointsPath = \"abfss://xxx.dfs.core.windows.net/deltaCheckpoints\"\n",
        "val deltaTablesPath = \"abfss://xxx.dfs.core.windows.net/deltaTables\"\n",
        "\n",
        "// connect to cosmos db streaming\n",
        "val dfStream = spark.readStream.\n",
        "    format(\"cosmos.oltp\").\n",
        "    option(\"spark.synapse.linkedService\", \"SynapseDb\").\n",
        "    option(\"spark.cosmos.container\", \"Tenant\").\n",
        "    option(\"spark.cosmos.changeFeed.readEnabled\", \"true\").\n",
        "    option(\"spark.cosmos.changeFeed.startFromTheBeginning\", \"true\").\n",
        "    option(\"spark.cosmos.changeFeed.checkpointLocation\", \"/localReadCheckpointFolder\").\n",
        "    option(\"spark.cosmos.changeFeed.queryName\", \"streamQuery\").\n",
        "    load()\n",
        "\n",
        "// do some data transformation\n",
        "val records = dfStream.\n",
        "    select(\n",
        "        from_unixtime($\"_ts\").cast(\"timestamp\").as(\"createTime\"),\n",
        "        $\"id\",\n",
        "        $\"aadTenantId\",\n",
        "        $\"country\",\n",
        "        $\"onboardTime\".cast(\"timestamp\"),\n",
        "        $\"offboardTime\".cast(\"timestamp\"),\n",
        "        $\"licenseStatus\"\n",
        "    ).\n",
        "    withWatermark(\"createTime\", \"10 minutes\")\n",
        "\n",
        "// save into delta tables\n",
        "records.\n",
        "    writeStream.\n",
        "    format(\"delta\").\n",
        "    outputMode(\"append\").\n",
        "    partitionBy(\"country\").\n",
        "    option(\"checkpointLocation\", deltaCheckpointsPath).\n",
        "    start(deltaTablesPath).\n",
        "    awaitTermination()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse Spark",
      "name": "synapse_spark"
    },
    "language_info": {
      "name": "scala"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
